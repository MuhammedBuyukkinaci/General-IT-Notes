# Designing Machine Learning Systems

# Overview of Machine Learning Systems

1) The book and its sections

![](./images/001.png)

2) It is possible to roll out an ML model without training it. Continual learning would be in this category. When data arrives, it will train.

3) Training data and unseen data should come from the same distribution.

4) ML is useful when the cost of a wrong prediction is low.

5) If ML can't solve our problem, we can break the problem into subproblems and apply ML to each subproblem.

6) When you type on your phone, the next word would be predicted via ML. This problem is known as predictive typing.

7) Price optimization is an ML problem happening in flight tickets, accomodation bookings, internet ads, ride sharing. ML based price optimization is mostly usef for businesses that have huge amount of transactions, changing demands and eager customers that can pay dynamic price.

8) Acquiring a new customer is 5 to 25 times more expensive than retaining an old customer.

9) Some ML problems:

- Support Ticket Classification
- Brand monitoring: Sentiment analysis problem.

10) ML in Research vs ML in production

![](./images/002.png)

11) It is crucial for ML engineers to understand requirements from all stakeholders.

12) Ensembling techniques aren't generally used in production due to high latency characteristic.

13) ML is different than traditional SWE. In SWE, data and code are aimed to be separated. Whereas, ML systems are composed of data, code and artifacts generated by data and code.

# Chapter 2. Introduction to Machine Learning Systems Design

1) One of the typical characteristic of a failed ML project is that data scientists focus on model metrics without paying enough attention to business metrics.

2) In CTR(Click Through Rate) and Fraud Detection, model metrics and business metrics align together. Therefore, they are the most popular ML problems.

3) Most ML systems should have the following characteristics:

    - Reliability
    - Scalability
    - Maintainability
    - Adaptability

4) Code, data, and artifacts should be versioned.

5) ML project lifecycle

![](./images/003.png)

6) Binary classification is simpler than multiclass classification.

7) It is a good practice to have at least 100 examples for each class in a multiclass classification setup.

8) When the number of classes is high, hierarchical classification is useful. Let's assume we work in an e-commerce website. We are trying to predict the subcategory of a product. Instead of predicting subgroup via single model, split the problem into two: the first ML model predicts product category as fashion, electronics, kitchen etc. Then, the second model is going to predict as skirt, t-shirt, jacket etc.

9) Multilabel classification can be handled in two ways:
    - One model, one sample can have many labels as positive. The target array is like (0,1,1,0) instead of (0,1,0,0).
    - A separate model for each label. COnverting the problem into multiclass.

10) Multilabel classification is hard to solve compared to multiclass classification and binary classification. The first reason is the ambiguity in labeling. One labeller might think differently than another labeller. Another reason is the interpretation of the output. In multiclass classification, we predict the category with the highest category. However, in a multilabel classification, we aren't sure to assign a label because multiple labels can be assigned.

11) Framing a problem might differ. The problem is next app opening. The latter approacih is more correct. The former is vulnerable to app delete, new installed app because it requires retraining. However, the second approach doesn't require retraining.

![](./images/004.png)

![](./images/005.png)

12) Pareto optimization is about optimizing several competing objectives at the same time. Let's assume we have an ML project. We want best model performance on test data, lower latency in inference, small model in RAM, no bias in model etc. These different objectives sometimes conflict with each other. Improving one can make another one worse. "Pareto Optimization finds a set of solutions where no objective can be improved without hurting at least one other objective."

13) Let's assume we want to feed a social media homepage using ML. We want high quality content to appear at the top and we want contents resulting in more engagement to appear at the top. These two objectives are goals of the project. In such a scenario, it is a good practice to decouple objectives. One model for quality score, one model for engagement score.

14) Data Science Hierarchy

![](./images/006.png)

# Chapter 3 - Data Engineering

1) Storing data is interesting if you are going to retrieve it later. To grab stored data, its structure is also as important as its format. "Data models define how the data stored in a particular data format is structured".

2) Migrating logs from a log service such as datadog or logstash to object storage such as s3 might be a cheaper alternative to preserve logs in the long run.

3) Different types of data:
- First party data: Data owned by the company
- Second party data: Data owned by another company, our company buy it from them.
- Third party data: Data obtained from another company, which sells data to our company.

4) Each phone used to have a unique advertiser ID.

5) Data serialization is the process of transforming a data structure or object state into a format which can be stored, transmitted or reconstructed later.

6) Common data formats and how they are used

![](./images/007.png)

7) JSON is text. Thus, it consumes a lot of space.

8) CSV is row-major, which means consecutive elements in a row are stored next to each other in memory. Parquet is column-major, which means consecutive elements in a column are stored next to each other in memory. Row-major formats is faster in data writes. Pandas library in Python is in columnar format. Accessing a variable by row in pandas is much slower than accessing a variable by column in numpy.

![](./images/008.png)

9) AWS recommends parquet over CSV. Parquet is 2x faster in unloading and consumes up to 6x less storage in S3, according to AWS study.

10) Data model tells us how data is represented. Let's assume we can store the information of a car with its model, its year and its color. We can also store its plate number, its address, its owner. These are 2 different data models. How data is stored matters a lot.

11) Python is an imperative language. Whereas, SQL is a declarative language.

12) [Ludwig](https://github.com/ludwig-ai/ludwig) and [H2o Auto ML](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) are 2 declarative ML frameworks.

13) Declarative ML is useful in development. However, things get complicated when moving into the production.

14) The first reason why some companies start using NoSQL is that schema management in RDBMS's is painful.

15) PostgreSQL and MySQL support both RDBMS and NoSQL.

15) There are 2 types of non-relational data models:
    - Document: The relationship between one document and another is rare. Document databases move the responsibility of assuming structures from apps writing to databases to apps reading from databases. Structure is cared in reading, not in writing. The con is that retrieving data such as filtering all items whose prices are less than 30 dollars. In order to do this, you should read all documents, extract price field, compare the price to 30 dollars. The content of a document is important.
    - Graph: Relationship between data items is common and important. The relationship between data items is important.

16) An example graph database

![](./images/009.png)

17) Data warehouses store structured data. Data lakes store unstructured data.

18) Transactional databases(OLTP db's) require low latency and high availability. These databases have ACID(Atomicity, Consistency, Isolation, Durability).
    - Atomicity: All transactions in a group must be completed successfully. If payment fails on Uber app, no ride should be ordered.
    - Consistency: All transactions must follow predefined rules.Only logged in users can order a ride on Uber app. 
    - Isolation: 2 transactions can be carried out at the same time if there are isolated. 2 people can't have the same taxi ordered on Uber app√ß
    - Durability: When a change is commited, it should persist even the system breaks. Let's assume we order a ride on Uber app and our phone died. The taxi should come to us.

19) OLTP isn't may not be a perfect fit for analytics. Thereforo, OLAP(Online Analytical Processing) emerged.

20) The separation between OLTP and OLAP was in the past. It is obsolete. Nowadays, OLTP databases like CoachRockDB can handle analyticaly queries. In addition, OLAP databases like Apache Iceberg and DuckDB can handle transactional queries.

21) It is a good practice to decouple storage from processing. We can store our data on PostgreSQL and we can process it on Apache Spark.

22) ETL illustration

![](./images/010.png)

23) ETL is much more used than ELT.

24) Data can move in 3 ways:

- Data passing through databases: Process A writes to a database. Process B reads it from the database. This way isn't feasible in latency-critic operations. Reading and writing operations cause to high latency in database operations.

- Data passing through services such as REST, RPC: Client-server architecture. REST is used generally by Public API's. RPC is used between services of the same company. RPC frameworks such as gRPC is designed to be utilized internally.

- Data passing through via a real time transport like Kafka and AWS Kinesis: a Real time transport can be thought as in-memory storage. Event driven design. Real time transport introduces asynchronous data passing with low latency. Request driven system is optimal for the systems relying more on logic rather than data. Event driven architecture(EDA) is optimal for the systems relying more on data rather than logic. EDA has 2 types: pub-sub(Kafka) and message-broker(RabbitMQ). Kafka is optimized for throughput, RabbitMQ is optimized for low latency. Kafka is for event streaming and large scale data pipelines. RabbitMQ is for tradional messaging, task queues, complex routing.

![](./images/011.png)

![](./images/012.png)

25) Data can be processed in 2 ways: 
    - Batch: Map Reduce and Spark are 2 batch processing methods.
    - Streaming: Useful for low latency. Kafka s an example to hos streaming data. Apache Flink is a streaming processing method. There is also Kafka Streaming processing option, but it is limited. Flink is highly scalable and fully distributed. The frequency of kicking off a job(once in 5 minutes) is more compared to batch(once a day). KSQL and Spark Streaming are also other streaming alternatives. Flink and KSQL are industry-standards and they provide SQL-alike abstraction for querying.

26) The features computed via batch processing are called static features. They are less frequently updated. Whereas, the features computed via streaming processing are called dynamic features. The average rating of an Uber driver is a static feature, computed via batch processing. How many rides to be completed in the next 2 minutes is a dynamic feature, computed via streaming processing.

# Chapter 4 - Training Data

1) Data is full of bias. Use data, but don't trust in it.

2) There are 2 ways of sampling: nonprabability sampling and random sampling.

3) Non probability sampling methods:

- **Convenience sampling**: Data is selected based on availability. This method is popular because it is convenient. Data scraped from wikipedia, reddit etc. and used in the training phase of LLM's are obtained via this method.

- **Snowball sampling**: Let's assume you want to scrape data on twitter. You choose a set of accounts. Scrape their tweets. Then, move to the accounts they follow. Do the same operation in the followp. Snowball effect. 

- **Judgment sampling**: Data is selected based on the decision of experts.

- **Quota sampling**: Data is selected based on quotas such as 30-60 aged individuals, under 30 people etc.


4) Random sampling:

- Simple random sampling: It is about sampling randomly. Its advantage is its ease of use. Its drawback is that minority classes can be discarded while sampling.

- Stratified sampling: It can be exemplified in this way: Let's assume there are 2 classes, class A and class B. You are sampling 1% of class A samples and 1% of class B samples. Its drawback is that it can't be implemented in multilabel classification.

- Weighted sampling: Some weights are assigned to samples while sampling. The more the weight, the more probable the sample is selected.

- Reservoir Sampling: It is useful in sampling streaming data.

![](./images/013.png)

- Importance sampling: Used in policy-based reinforcement learning.

5) Different labeling methods:

- Hand Labeling: Expensive and slow but accurate. It is a good practice to version data. For example, we trained a model using data 1. After that, we want hand labellers to label more data. Then, we concatenated data 1 and data 2 to train a newer model. However, the model performance detoriated. In this scenario, it would better to version data to detect performance shift. This versioning is known as data lineage.

- Natural Labeling: Label exists in data. For example, ETA of Google Maps, stock price prediction, recommendation engine have the label themselves. Not need to be labeled.

6) Many recommender systems have low feedback durations. Let's assume we showed a list of products to a user. After the user views the widget,  we can obtain the labels as whether the user clicked or not afterwards.

7) Some recommender systems aim to optimize click. Some recommender systems aim to optimize purchase. Both approaches are valid.

8) Fraud detection has long feedback loop. A transaction might be reported as fraudelent in 1 to 3 months.

9) Some techniques to handle lack of labels.

![](./images/014.png)

- Weak Supervision: It is also known as programatic labeling. It is around labeling function(LF). It extracts labels from data according to some specific patterns such as whether a note has "heart attack" in nurse note, whether the texts matches a specific regex message, database lookup, the output of other models etc. It is a good practice to validate weak supervision on a labeled data. It is a good practice to combine several LF's. [Snorkel](https://github.com/snorkel-team/snorkel) is a library used for this purpose. It is simple but powerful. However, it is not perfect. In the following second image, the performances of weak supervision and a radiologist are compared. They appeared close when data points increases.

![](./images/015.png)

![](./images/016.png)

- Semi Supervision: It is different than weak supervision. We have a limited labeled dataset. It is especially useful when the number of training records is limited. It has different implementations:

    - Self Training: We train a model on top of limited data. Then, we predict the unlabeld samples. Pick the predicted labels of records whose predictions are confident. Iterate over this.

    - Similar characteristics have the same data: Let's assume we have a tweet like "The department of computer science #AI #ML # NLP". We labeled #AI as **computer science**. We can also label #ML and #NLP as **computer science** too.

    - Perturbation Based Model: It is based on the assumption of adding a small noise to a record doesn't change its label. An example is adding noise to a training record in computer vision, adding noise to the embedding represantion of a token in NLP.

